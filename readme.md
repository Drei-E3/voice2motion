# Robotic Arm Middleware

A modular NLP-to-motion pipeline that interprets natural language commands and translates them into robotic arm control actions, with environmental awareness and reasoning via a local LLM.

---

## ğŸ§  Project Overview

This middleware connects natural language input to a robotic control system. It performs:

1. **Intent Detection** â€“ Determines whether input requires robotic action.
2. **Object Matching** â€“ Parses environment data and finds relevant objects.
3. **LLM Reasoning** â€“ Uses a local LLM to generate a chain-of-thought plan.
4. **Feedback Loop** â€“ Handles uncertainty via clarification or rejections.
5. **Output Generation** â€“ Produces structured action steps (`Output.json`).

---

## ğŸ—ï¸ Middleware architecture 

![architecture](docs/Architecture.png)
---

## ğŸ“ Project Structure

```
robotic-arm-middleware/
â”‚
â”œâ”€â”€ app.py                       # Entry point of the middleware
â”‚
â”œâ”€â”€ input\_layer/
â”‚   â””â”€â”€ intent\_filter.py         # Filters whether command targets robotic arm
â”‚
â”œâ”€â”€ middleware/
â”‚   â””â”€â”€ pipeline.py              # Core logic: match, reasoning, output
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ env\_loader.py            # Environment.json loader
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ environment.json         # Current scene objects (input)
â”‚   â””â”€â”€ output.json              # Generated step-by-step plan (output)
â”‚
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ dummy\_llm.py             # Placeholder LLM call logic (replaceable)
â”‚
â””â”€â”€ README.md                    # You're reading it
```
---

## ğŸ”§ Requirements

- Python 3.8+
- `openai` or your own LLM client SDK
- Your local model endpoint (e.g. LM Studio, Ollama)

Install dependencies:

```
pip install openai
```

---

## ğŸš€ How to Run

1. Place your `environment.json` in `data/`.

2. Run the app application:

```
python app.py
```

3. Enter a command:

```
move the handy next to the fruit
```

4. Generated plan will appear in `data/output.json`.

---

## ğŸ”„ Replace Dummy LLM

Update `llm/dummy_llm.py` to point to your local/remote LLM API:

```
client = OpenAI(base_url="http://localhost:1234/v1", api_key="your-key")
# for LM Studio
# client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")
response = client.chat.completions.create(...)
```

---

## ğŸ“Œ TODO

* [ ] Add speech-to-text preprocessing layer
* [ ] Collision detection refinement
* [ ] Neurapy arm API integration

---

## ğŸ“œ License

MIT License Â© 2025